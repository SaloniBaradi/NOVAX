# -*- coding: utf-8 -*-
"""NOVAX

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DP3akjevv0OONdv2oWOAg9lIPGXNcN7K
"""

!pip install transformers accelerate sentencepiece bitsandbytes

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_name = "ibm-granite/granite-3b-instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

prompt = "Explain what AI is in simple words."

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=200)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

from huggingface_hub import login

login()

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

repo_id = "ibm-granite/granite-3.3-2b-instruct"

tokenizer = AutoTokenizer.from_pretrained(repo_id)

model = AutoModelForCausalLM.from_pretrained(
    repo_id,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

def ask(question):
    inputs = tokenizer(question, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        do_sample=True,
        temperature=0.7
    )
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))

ask("Who are you?")

from google.colab import files
uploaded = files.upload()

from pypdf import PdfReader

pdf = PdfReader("sample_study.pdf")

text = ""
for page in pdf.pages:
    text += page.extract_text() + "\n"

print("PDF Loaded Successfully!")
print(text[:1000])  # preview first 1000 characters

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_id = "mistralai/Mistral-7B-Instruct-v0.2"

tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.float16
)

print("Mistral 7B Loaded Successfully!")

def ask_studymate(question, context):
    prompt = f"Context:\n{context}\n\nQuestion: {question}\nAnswer:"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    output = model.generate(**inputs, max_new_tokens=200)
    return tokenizer.decode(output[0], skip_special_tokens=True)

response = ask_studymate("Explain ISA in simple words.", text)
print(response)